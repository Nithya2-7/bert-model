# BERT Tokenizer and Encoder

This project demonstrates how to use a pre-trained BERT language model (`bert-base-uncased`) from Hugging Face to:
- Tokenize input text (split into subwords).
- Encode it into numeric IDs suitable for model input.
- Extract vector representations (embeddings) from BERT.

This is a core task in any NLP pipeline and can be integrated into larger curriculum-based NLP projects such as sentiment analysis, chatbot development, or document classification.

install dependencies
pip install -r requriments.txt
